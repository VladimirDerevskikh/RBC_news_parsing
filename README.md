Данный проект предназначен для скачивания новостей с сайта РБК по заданному тегу и датам начала и конца интересующего периода с последующим их 
сохранением в 'csv' файле. У каждой новости в результате парсинга определяется пять компонентов - время, дата, заголовок, аннотация и текст, 
которым соответствуют столбцы этого 'csv' файла с новостями, а его строки соответствуют скачиваемым новостям. Проект включает в себя файл основной 
программы "rbc_news_parsing.py" и модуль "news.py" и ниже приводится описание того, как они устроены и работают, обеспечивая выполнение задачи 
проекта. Сначала будет описан модуль "news.py", а затем файл основной программы. Код можно посмотреть непосредственно в самих этих файлах. В них 
помимо кода имеется небольшое количество поясняющих комментариев, а полное объяснение кода данного проекта дается в настоящей документации 
"readme.txt". Кроме того, выложен файл 'rbc_Санкт-Петербург_10_09_2020-19_09_2020.csv', содержащий результаты работы программы. При отображении
в Excel могут возникнуть проблемы с кодировкой по умолчанию, так как в данной программе сохранение производится в кодировке "koi-8" (для того, 
чтобы правильно записывались всевозможные символы, например, знак рубля), а в Excel может быть другая кодировка и для правильного отображения надо 
будет открыть файл с кодировкой "koi-8". Просматривать записи из этого файла можно с помощью такого кода Python:

import pandas as pd

n = 120
df = pd.read_csv('rbc_Санкт-Петербург_10_09_2020-19_09_2020.csv', encoding = 'utf-8')

print(df.iloc[n]['Time'])
print(df.iloc[n]['Date'])
print(df.iloc[n]['Header'])
print(df.iloc[n]['Overview'])
print(df.iloc[n]['Text'])

Всего в этом файле 142 новости, поэтому n можно задавать от 0 до 141.

Каждая новость, содержащаяся в результатах поиска на сайте РБК по заданному тегу в заданном промежутке времени (отображаются ссылки-заголовки и 
часть текста, а новости содержатся на страницах по этим ссылкам), преобразуется в объект класса News, который определяется в модуле "news.py", 
описанном ниже.

Как уже говорилось, в модуле "news.py" определяется класс News. Объект такого класса по сути представляет собой контейнер, содержащий компоненты 
текстового содержимого страницы с новостью. Конструктор этого класса получает ссылку на страницу с новостью и с помощью конструктора BeautifulSoup 
создает из содержимого этой страницы объект BeautifulSoup (содержимое страницы получается с помощью метода "text" объекта ответа, являющегося 
результатом функции get() из библиотеки "requests", примененной к ссылке на страницу с новостью). Этот код выглядит так:

news_f = bs(requests.get(link).text, "html.parser"), где 'bs' это сокращенное наименование библиотеки BeautifulSoup, импортированной в начале 
модуля "news.py". Далее с помощью полученного объекта BeautifulSoup определяются следующие атрибуты класса, соответствующие пяти компонентам 
новости:

date (дата публикации новости), time (время публикации новости), header (заголовок новости), overview (аннотация к новости), text (текст новости).

Для получения компонентов date и time сначала применяется метод find(), который отыскивает тег с атрибутом itemprop="datePublished". Если такой 
тег находится, то затем берется значение атрибута "content" этого же тега, от этого значения отбрасываются последние 6 символов, представляющие 
собой величину сдвига от Гринвича, и оставшаяся часть разбивается методом split() с разделителем 'T' на две части - дату и время, которые и 
записываются в атрибуты класса "date" и "time". Если же такого тега не находится, то в атрибуты "date" и "time" записывается значение 'N/A'. 
Аналогичный принцип применяется и для других атрибутов - если соответствующие данные отсутствуют на странице с новостью, то в атрибут записывается 
значение 'N/A' - например, если у новости нет аннотации, то в атрибут "overview" будет записано значение 'N/A'.

Компоненты новости "заголовок", "аннотация" и "текст" содержатся внутри тегов, у которых атрибут "class" имеет значения "article__header__title", 
"article__text__overview" и "article__text" соответственно. Для их получения в модуль "news.py" была включена отдельная функция text_extractor, 
которая на вход принимает список объектов "Tag", являющийся результатом применения метода find_all() у описанного выше объекта "news_f" класса 
BeautifulSoup, и извлекает из него текстовое содержимое соответстующего компонента. Этот метод каждый раз вызывается с соответствующим значением 
именованного аргумента "class_" (используется такое наименование, а не "class", так как "class" является зарезервированным словом в Python), чтобы 
получить объекты "Tag", содержащие заголовок, аннотацию или текст новости. Если таких объектов не находится, то в соответствующий атрибут 
записывается  значение 'N/A', а при положительном результате поиска - записывается результат действия функции text_extractor(). Кроме того, в 
случае текста статьи, если после обработки списка объектов "Tag" остается пустая строка, то в атрибут "text" также записывается значение 'N/A'.

Теперь о функции text_extractor()

Как уже говорилось выше - на вход она принимает список объектов "Tag" (по сути - это теги с соответствующими значениями атрибута "class"), а затем 
извлекает из него все необходимое текстовое содержимое. Для каждого объекта "Tag" из этого списка сначала выбираются все непосредственные дочерние 
элементы, для чего создается основной цикл по списку и в этом цикле сначала переменной inner_tags присваивается результат метода find_all(), 
вызываемого у каждого объекта "Tag" из списка, причем вызов производится с аргументом recursive = False, чтобы извлекались только непосредственные 
потомки тега (теги, являющиеся полученными при этом вызове непосредственными потомками, будут называться тегами первого уровня вложенности). Затем 
производится перебор по этим непосредственным потомкам. Как показал анализ страниц с новостями - среди тегов первого уровня вложенности имеются
теги с текстом, который не надо включать в формируемый компонент новости. Такими тегами являются все теги <div>, кроме тех, у которых атрибут 
"class" имеет значение 'article__text__pro' или 'article__special_container'. Поэтому при переборе тегов первого уровня вложенности сначала 
ставится условие, заключающееся в том, чтобы имя тега не было "div" или (если оно "div") значение атрибута "class" присутствовало в списке 
"classes_included". В самом начале функции text_extractor() был определен список, содержащий значения атрибута "class" у тегов <div> с полезным 
текстом (который надо включать в формируемый компонент новости):

classes_included = ['article__text__pro', 'article__special_container']

Код был организован именно таким образом, на тот случай, если в будущем обнаружатся страницы, в которых полезный текст содержится в тегах <div>
первого уровня вложенности, у которых атрибут 'class' имеет какое-либо еще значение помимо этих двух. Анализ страниц с новостями также показал, 
что внутри содержащих полезный текст тегов первого уровня вложенности иногда содержатся теги с ненужным текстом. Такими тегами являются теги с 
наименованием 'div', 'script' или 'span' и при этом не содержащие внутри себя тега <p>. Эти три наименования тегов также включены в список 
"tags_excluded" в начале функции text_extractor(), на тот случай, если в будущем обнаружатся страницы с имеющими иное наименование тегами с 
ненужным текстом, чтобы облегчить добавление нового наименования.

Для исключения ненужного текста внутри тегов первого уровня вложенности после условия, исключающего ненужные теги <div>, проводится перебор по
результатам вызова метода find_all() у каждого тега первого уровня вложенности, удовлетворяющего этому условию. Для каждого дочернего элемента,
получаемого в результате этого перебора, делается проверка того, что его наименование содержится в списке "tags_excluded", но при этом он не 
содержит внутри себя тега <p>. Если это условие выполняется, то такие элементы заменяются на пустую строку, благодаря чему внутри обрабатываемых 
тегов первого уровня вложенности остается лишь нужный текст. Этот код выглядит вот так

...
        if (tag.name != 'div' or tag['class'][0] in classes_included):
            for sub_tag in tag.find_all():
                if sub_tag.name in tags_excluded and not sub_tag.find('p'):
                    sub_tag.replace_with('')
...


Далее идет проверка того, что обрабатываемый тег первого уровня вложенности (с уже исключенным ненужным текстом) имеет какой-либо текст помимо 
пробелов и если это так, то в переменную result (она была определена пустой строкой в начале функции text_extractor()) прибавляется текстовое 
содержимое этого обрабатываемого тега. Для получения этого содержимого используется метод get_text() объекта "Tag" и строковый метод strip (), а 
кроме того, для удаления излишних пробелов внутри него и замены групп пробельных символов на один символ '\n' применяются строковые методы split() 
и join() и замена с использованием регулярного выражения (метод sub() модуля "re"). Также в конец извлеченного текстового содержимого добавляется 
символ переноса строки '\n' для удобства отображения итогового текста (содержащегося внутри тегов, подаваемых на вход функции text_extractor()) с 
помощью функции print(). После выполнения всех этих операций для каждого тега певого уровня вложенности функция text_extractor() возвращает 
переменную result, содержащую полезный текст из элементов обрабатываемого списка объектов "Tag".


Помимо конструктора класс News также содержит следующие методы 

as_dict - служит для представления экземпляра класса в виде словаря с ключами 'Date', 'Time', 'Header', 'Overview' и 'Text' и значениями, 
получаемыми из соответствующих атрибутов экземпляра. Такие словари применяются при создании объекта pandas.DataFame, используемого для сохранения 
новостей в 'csv' файле.

save_to_csv - служит для сохранения новости (экземпляра класса) в 'csv' файл. Сохранение производится посредством формирования из атрибутов 
экземпляра класса однострочной таблицы pandas.DataFrame с заголовком, содержащим соответствующие наименования атрибутов 'Date', 'Time', 'Header', 
'Overview' и 'Text', с последующим применением метода pandas.DataFrame.to_csv(). Обеспечена возможность использования тех же параметров, что и в 
методе to_csv() у pandas.DataFrame, при этом следующие три параметра задаются по умолчанию: header = False, index = False и mode = 'a'. Это 
делается потому, что в дальнейшем этот метод предполагается в основном использовать для дописывания одной новости в уже существующий файл с 
набором новостей (поэтому при записи отбрасывается заголовок таблицы pandas.DataFrame и запись осуществляется в режиме 'append'), а также для 
исключения записи индекса при сохранении (в основном файле программы новости также сохраняются без записи индекса).


Помимо упомянутых уже класса News и функции text_extractor() модуль "news.py" содержит функцию url_filename_composer(). Она, используя тег и даты
начала и конца периода, в соответствии с которыми необходимо произвести поиск новостей на сайте РБК, создает ссылку, с помощью которой можно 
отправить соответствующий запрос GET к серверу сайта РБК, а также наименование файла для сохранения новостей, которое отражает тег и период поиска 
новостей (например, 'rbc_Санкт-Петербург_10_09_2020-19_09_2020.csv'). Эта функция имеет следующие аргументы:

tag (значение по умолчанию - 'Роснефть')    - тег, по которому ищутся новости.
offset (значение по умолчанию - 0)          - смещение (про этот параметр будет сказано ниже при пояснении работы основной программы).
start_date (значение по умолчанию - None)   - если при вызове функции этот аргумент не передается, то в него записывается значение на неделю
                                              меньше значения аргумента end_date.
end_date (значение по умолчанию - None)     - если при вызове функции этот аргумент не передается, то в него записывается текущая дата.


Теперь об основной программе.

Для получения ссылок с новостями выполняется запрос GET, на который возвращается ответ в формате JSON, содержащий словарь с одним ключом "items", 
которому соответствует значение в виде списка словарей, каждый из которых соответствует отдельной новости и содержит ссылку на эту новость в 
значении при ключе "fronturl". Для выполнения такого запроса используется ссылка следующего вида (для ее формирования используется упомянутая выше 
функция url_filename_composer() из модуля "news.py"):

https://www.rbc.ru/v10/search/ajax/?project=rbcnews&dateFrom=10.08.2020&dateTo=19.09.2020&offset=0&limit=10&query="Роснефть"

В этой ссылке определяются следующие параметры запроса:
dateFrom - начало периода поиска новостей.
dateTo   - конец периода поиска новостей.
offset   - смещение или после какой по счету из найденных новостей они включаются в ответ (применяется обратный хронометраж, то есть 
           первыми идут более поздние новости).
limit    - предельное количество новостей, получаемых в ответе (в этой программе этот параметр всегда будет задаваться равным 10).
query    - тег, по которому осуществляется описк новостей.

Сначала в программе производится необходимое импортирование (библиотеки "requests" и "pandas" и написанный для этой программы модуль "news.py").
Затем задаются параметры поиска: 

tag        - тег, по которому ищутся новости.
start_date - начало периода поиска новостей.
end_date   - конец периода поиска новостей.

А также определяется переменная offset, соответствующая одноименному параметру запроса GET. Сначала она задается равной нулю, а затем будет 
претерпевать изменения внутри цикла while, о чем будет сказано ниже. Затем задается пустой список links, который будет наполняться ссылками на 
страницы с новостями.

После этого с помощью функции url_filename_composer() из модуля "news.py" задается ссылка для отправки запроса GET с параметрами, соответствующими
значениям переменных tag, start_date, end_date и offset (она записывается в переменную url), и учитывающее эти параметры наименование файла, в 
который будут сохраняться найденные новости (записывается в переменную filename). Затем полученная ссылка передается в функцию get() из библиотеки 
"requests", а у полученного объекта ответа вызывается метод json(), чтобы получить представление ответа в формате JSON, и в этом словаре берется 
значение по ключу 'items' и записывается в переменную items.

Далее программа переходит к циклу while. Его условием является отличие переменной items от пустого списка. Внутри цикла из каждого элемента списка 
items извлекается и дописывается в список links ссылка на новость, содержащаяся в значении при ключе 'fronturl' (кроме того, предприняты 
необходимые действия на случай, если начальная часть этого значения отличается от "https:", как иногда бывает - в этом случае в начало извлекаемой 
строки добавляется "https:"). Затем значение переменной offset увеличивается на 10, с помощью функции url_filename_composer() из модуля "news.py" 
текущее значение переменной url немного изменяется (параметр "offset" соответственно увеличивается на 10, а остальные параметры остаются 
неизменными), полученные по новому запросу GET данные записываются в переменную items и действие переходит в начало цикла while. Таким образом, 
список links либо остается пустым, если на сервере сайта РБК вообще отсутвуют новости, удовлетворяющие заданным тегу поиска и началу и концу 
периода поиска, либо этот список заполняется ссылками на новости за один или более проходов цикла while (условно говоря - блоками по 10 новостей и 
последний (или единственный) блок может содержать от 1 до 10 новостей).

После цикла while, если список links не пустой, из него с помощью генератора списка создается список соответствующих новостям словарей. Для каждой 
ссылки из списка links создается объект класса News с атрибутами, соответствующими компонентам находящейся по ссылке новости, который с помощью
метода as_dict() преобразуется в словарь. Затем этот список словарей преобразуется в объект pandas.DataFrame со столбцами, соответствующими 
компонентам новостей (columns = ['Date', 'Time', 'Header', 'Overview', 'Text']), который с помощью метода to_csv() сохраняется в 'csv' файл с 
наименованием, соответствующим параметрам поиска новостей (переменная filename). На этом программа завершает свою работу.

Код проекта был написан с использованием Python и библиотек следующих версий:
Python:    3.7.4
requests:  2.22.0
pandas:    0.25.1

